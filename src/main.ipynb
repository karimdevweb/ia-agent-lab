{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edc6a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "#  load the api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "316c3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load the api keys\n",
    "load_dotenv(\"APIKeys.env\")\n",
    "GeminiKey = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c58b8",
   "metadata": {},
   "source": [
    "## 1- first start by calling Gemini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b72aa844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a function to automate the call\n",
    "def CallGeminiModal(GeminiKey, query):\n",
    "\n",
    "    client = genai.Client(api_key=GeminiKey)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=query,\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a201e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ask a question\n",
    "Geminiquery = \"do you know Einstein\"\n",
    "GeminiRes = CallGeminiModal(GeminiKey, Geminiquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca91eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't \"know\" anyone in the personal, human sense of having met them or experiencing a relationship.\n",
      "\n",
      "However, I have access to a vast amount of information *about* Albert Einstein! I can tell you all about:\n",
      "\n",
      "*   His life and biography\n",
      "*   His groundbreaking scientific theories (special and general relativity)\n",
      "*   His famous equation (E=mc²)\n",
      "*   His Nobel Prize-winning work on the photoelectric effect\n",
      "*   His impact on physics and the world\n",
      "*   And much more!\n",
      "\n",
      "So, in the sense of having extensive knowledge about him, then yes, I am very familiar with Albert Einstein.\n"
     ]
    }
   ],
   "source": [
    "# import all libraries\n",
    "# import API key\n",
    "# define function calling the model\n",
    "# define the query/the question\n",
    "# catch and print the result\n",
    "print(GeminiRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d907a",
   "metadata": {},
   "source": [
    "## 2- seconde, try ollama gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "176b5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import ollama\n",
    "# define a function\n",
    "def CallGptModel(model, role, query):\n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\n",
    "                'role': role,\n",
    "                'content': query,\n",
    "            },\n",
    "        ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ee2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-oss:latest\"\n",
    "role= \"user\"\n",
    "query = \"do you know Einstein\"\n",
    "GptRes = CallGptModel(model, role, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba53425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes—I’m familiar with Albert Einstein! He was a German‑born physicist (1879‑1955) who revolutionized our understanding of space, time, and gravity. His most famous contributions include:\n",
      "\n",
      "| Topic | What he did |\n",
      "|-------|-------------|\n",
      "| **Special Relativity** (1905) | Showed that the speed of light is constant in all inertial frames, leading to the famous relation \\(E=mc^2\\). |\n",
      "| **General Relativity** (1915) | Extended relativity to include gravity, describing it as the curvature of spacetime caused by mass‑energy. |\n",
      "| **Photoelectric Effect** (1905) | Demonstrated that light comes in quanta (photons), earning him the 1921 Nobel Prize in Physics. |\n",
      "| **Brownian Motion** (1905) | Provided experimental evidence for the existence of atoms. |\n",
      "| **Bose–Einstein Condensate** (1925) | Collaborated with Satyendra Nath Bose on quantum statistics for identical particles. |\n",
      "\n",
      "Beyond physics, Einstein was a passionate advocate for peace, civil rights, and the scientific method. He spent the later years of his life in the United States, working on nuclear research and expressing skepticism about the nuclear arms race. If you’d like more details—on a specific paper, his life story, or his influence on modern science—just let me know!\n"
     ]
    }
   ],
   "source": [
    "print(GptRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a64532",
   "metadata": {},
   "source": [
    "## 3- third, try ollama mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d5854e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import ollama\n",
    "# define a function\n",
    "def CallMistralModel(model, role, query):\n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\n",
    "                'role': role,\n",
    "                'content': query,\n",
    "            },\n",
    "        ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9fad7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral:latest\"\n",
    "role= \"user\"\n",
    "query = \"do you know Einstein\"\n",
    "MistralRes = CallMistralModel(model, role, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1595ed63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, Albert Einstein was a famous physicist who is best known for developing the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). He is also known for his famous mass-energy equivalence formula E=mc^2. Einstein received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect,\" which was pivotal in establishing quantum theory. He is widely considered one of the greatest physicists in history.\n"
     ]
    }
   ],
   "source": [
    "print(MistralRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f25de0",
   "metadata": {},
   "source": [
    "# II- prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f64b9",
   "metadata": {},
   "source": [
    "### II 1.Gemini model with prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5657ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8539837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt engineering\n",
    "role = \"Tu brilles au ciel mais n’es pas une étoile.\"\n",
    "context = \"il y'a des soirs où tu montres ton visage, il y'a des soirs où t'es de profil.\"\n",
    "todo = \"Devine tout simplement qui tu es.\"\n",
    "format = \"En 2 phrases, décris qui tu es avec poésie ou rythme.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4c0a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini with parameters\n",
    "def GeminiWithParam(GeminiKey, role, context, todo, format):\n",
    "\n",
    "    client = genai.Client(api_key=GeminiKey)\n",
    "\n",
    "    # Construire un prompt unique à partir des \"variables\"\n",
    "    prompt = f\"{role}\\n{context}\\n{todo}\\n{format}\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4db6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model gemini with all params , prompt engineering\n",
    "GeminiParamRes = GeminiWithParam(GeminiKey,role, context, todo, format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54c10809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu es **La Lune**.\n",
      "\n",
      "Miroir d'argent, mon visage se dévoile, rond et plein, ou s'affine en un profil délicat. Silencieuse sentinelle du ciel, je berce vos nuits d'un éclat doux et mystérieux.\n"
     ]
    }
   ],
   "source": [
    "# print response\n",
    "print(GeminiParamRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd54eb0",
   "metadata": {},
   "source": [
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a5e9cd",
   "metadata": {},
   "source": [
    "### II- 2.gpt model - with prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98898b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import ollama\n",
    "# define a function\n",
    "def OllamaWithParam(model, role, context, todo , format):\n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\n",
    "                'role': role,\n",
    "                'content': context,\n",
    "                'todo': todo,\n",
    "                'format': format\n",
    "            },\n",
    "        ])\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa4b7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt engineering\n",
    "role = \"Tu brilles au ciel mais n’es pas une étoile.\"\n",
    "context = \"il y'a des soirs où tu montres ton visage, il y'a des soirs où t'es de profil.\"\n",
    "todo = \"Devine tout simplement qui tu es.\"\n",
    "format = \"En 2 phrases, décris qui tu es avec poésie ou rythme.\"\n",
    "model = \"gpt-oss:latest\"\n",
    "\n",
    "GptParamRes = OllamaWithParam(model, role, context, todo, format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc68d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result\n",
    "print(GptParamRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d461072",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ec4d7",
   "metadata": {},
   "source": [
    "### II- 3.Mistral model with prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "084d18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  call the model mistral\n",
    "modelMistral = \"mistral:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fc9acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the new params to teh function calling Ollama models\n",
    "MistralParamRes = OllamaWithParam(modelMistral, role, context, todo , format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: Let w(h) = 12 - 5*h. What is the least common multiple of 8 and w(-6)?\n",
      "Answer: 40\n"
     ]
    }
   ],
   "source": [
    "#  print the result\n",
    "print(MistralParamRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2e5700",
   "metadata": {},
   "source": [
    "### II. 4 trying with only one text foramt for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410166e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt engineering\n",
    "role = \"Tu brilles au ciel mais n’es pas une étoile.\"\n",
    "context = \"il y'a des soirs où tu montres ton visage, il y'a des soirs où t'es de profil.\"\n",
    "todo = \"Devine tout simplement qui tu es.\"\n",
    "format = \"En 2 phrases, décris qui tu es avec poésie ou rythme.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"{role}\\n{context}\\n{todo}\\n{format}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama with one prompt\n",
    "def OllamaOnePromptParam(model, prompt):\n",
    "    response = ollama.chat(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\": prompt\n",
    "            },\n",
    "        ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "481f1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "Gptmodel = \"gpt-oss:latest\"\n",
    "GptOneParam = OllamaOnePromptParam(Gptmodel, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "409aee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je suis la lune, éclat argenté qui parcourt le ciel nocturne.  \n",
      "Parfois je révèle mon visage complet, d’autres fois je me voile en profil, douce silhouette de lumière.\n"
     ]
    }
   ],
   "source": [
    "#  print result\n",
    "print(GptOneParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9aaea",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  call the model mistral\n",
    "modelMistral = \"mistral:latest\"\n",
    "MistralOneParam = OllamaOnePromptParam(modelMistral, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lune blanche dans le ciel sans attache,\n",
      "Ton regard change avec les variations du temps.\n",
      "\n",
      "Je suis la lune, une mystérieuse lumière enchantée, changiante et constante.\n"
     ]
    }
   ],
   "source": [
    "#  print result\n",
    "print(MistralOneParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1575ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fb867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to initialize device PRN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to initialize device PRN\n"
     ]
    }
   ],
   "source": [
    "! print(\"helo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7dd6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess \n",
    "def reactive_agent(prompt): \n",
    "    r = subprocess.run([\"ollama\",\"run\",\"mistral\",prompt],capture_output=True,text=True) \n",
    "    return r.stdout \n",
    "while True: \n",
    "    q=input(\"Vous : \") \n",
    "    if q.lower() in [\"quit\",\"exit\"]:break \n",
    "    print(\"Agent :\",reactive_agent(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26896a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac8b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ca69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebfe92a9",
   "metadata": {},
   "source": [
    "## III- Agent Reactif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23778e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1/\",  # Ollama local server\n",
    "    api_key=\"ollama\",  # obligatoire pour la lib OpenAI, mais ignoré localement\n",
    ")\n",
    "\n",
    "def reactive_agent(prompt):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"mistral:latest\",\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "print(\"Tapez 'exit' ou 'quit' pour arrêter.\\n\")\n",
    "\n",
    "while True:\n",
    "    q = input(\"Vous : \")\n",
    "    if q.lower() in [\"quit\", \"exit\"]:\n",
    "        break\n",
    "    print(\"Agent :\", reactive_agent(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c9104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fef54ec",
   "metadata": {},
   "source": [
    "## IV- Mini Chat-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Connexion à Ollama local\n",
    "try:\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1/\",  # Ollama local server\n",
    "        api_key=\"ollama\",  # obligatoire pour la lib OpenAI, mais ignoré localement\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"[Erreur] Impossible de connecter au serveur Ollama : {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "def reactive_agent(prompt, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Envoie un message à Mistral avec une température donnée.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"mistral:latest\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Erreur] Le modèle n’a pas pu répondre : {e}\"\n",
    "\n",
    "\n",
    "print(\"Tapez 'exit' ou 'quit' pour arrêter.\")\n",
    "print(\"Vous pouvez ajuster la créativité avec une température entre 0 et 1 (ex: 0.3 = plus logique, 0.9 = plus créatif)\\n\")\n",
    "\n",
    "# Température initiale\n",
    "temperature = 0.1\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        q = input(\"Vous : \")\n",
    "        if q.lower() in [\"quit\", \"exit\"]:\n",
    "            print(\"Fin du chat. À bientôt !\")\n",
    "            break\n",
    "\n",
    "        # Permettre de changer la température dynamiquement\n",
    "        if q.lower().startswith(\"temp \"):\n",
    "            try:\n",
    "                new_temp = float(q.split()[1])\n",
    "                if 0 <= new_temp <= 1:\n",
    "                    temperature = new_temp\n",
    "                    print(f\"Température mise à jour : {temperature}\")\n",
    "                else:\n",
    "                    print(\"Entrez une valeur entre 0 et 1.\")\n",
    "            except ValueError:\n",
    "                print(\"Format incorrect. Exemple : temp 0.8\")\n",
    "            continue\n",
    "\n",
    "        print(\"Agent :\", reactive_agent(q, temperature))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nChat interrompu par l’utilisateur.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"[Erreur inattendue] {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e8317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
